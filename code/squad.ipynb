{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_854624/410795828.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = datasets.load_metric('squad')\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "metric = datasets.load_metric('squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computes SQuAD scores (F1 and EM).\n",
      "Args:\n",
      "    predictions: List of question-answers dictionaries with the following key-values:\n",
      "        - 'id': id of the question-answer pair as given in the references (see below)\n",
      "        - 'prediction_text': the text of the answer\n",
      "    references: List of question-answers dictionaries with the following key-values:\n",
      "        - 'id': id of the question-answer pair (see above),\n",
      "        - 'answers': a Dict in the SQuAD dataset format\n",
      "            {\n",
      "                'text': list of possible texts for the answer, as a list of strings\n",
      "                'answer_start': list of start positions for the answer, as a list of ints\n",
      "            }\n",
      "            Note that answer_start values are not taken into account to compute the metric.\n",
      "Returns:\n",
      "    'exact_match': Exact match (the normalized answer exactly match the gold answer)\n",
      "    'f1': The F-score of predicted tokens versus the gold answer\n",
      "Examples:\n",
      "\n",
      "    >>> predictions = [{'prediction_text': '1976', 'id': '56e10a3be3433e1400422b22'}]\n",
      "    >>> references = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}]\n",
      "    >>> squad_metric = datasets.load_metric(\"squad\")\n",
      "    >>> results = squad_metric.compute(predictions=predictions, references=references)\n",
      "    >>> print(results)\n",
      "    {'exact_match': 100.0, 'f1': 100.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metric.inputs_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = datasets.load_metric('/data/ephemeral/level2-nlp-mrc-nlp-06/code/squad_generative.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nComputes SQuAD scores (F1 and EM).\\nArgs:\\n    predictions: List of question-answers dictionaries with the following key-values:\\n        - \\'id\\': id of the question-answer pair as given in the references (see below)\\n        - \\'prediction_text\\': the text of the answer\\n    references: List of question-answers dictionaries with the following key-values:\\n        - \\'id\\': id of the question-answer pair (see above),\\n        - \\'answers\\': a Dict in the SQuAD dataset format\\n            {\\n                \\'text\\': list of possible texts for the answer, as a list of strings\\n                \\'answer_start\\': list of start positions for the answer, as a list of ints\\n            }\\n            Note that answer_start values are not taken into account to compute the metric.\\nReturns:\\n    \\'exact_match\\': Exact match (the normalized answer exactly match the gold answer)\\n    \\'f1\\': The F-score of predicted tokens versus the gold answer\\nExamples:\\n\\n    >>> predictions = [{\\'prediction_text\\': \\'1976\\', \\'id\\': \\'56e10a3be3433e1400422b22\\'}]\\n    >>> references = [{\\'answers\\': {\\'answer_start\\': [97], \\'text\\': [\\'1976\\']}, \\'id\\': \\'56e10a3be3433e1400422b22\\'}]\\n    >>> squad_metric = datasets.load_metric(\"squad\")\\n    >>> results = squad_metric.compute(predictions=predictions, references=references)\\n    >>> print(results)\\n    {\\'exact_match\\': 100.0, \\'f1\\': 100.0}\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.inputs_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
